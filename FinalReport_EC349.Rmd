# EC 349 - Data Science for Economists Final Report

## Developing a model to predict Yelp Reviews

***Name: Bernardo Serrano***



**Introduction**

Star reviews are a way in which we attempt to quantify and standardize something as subjective as opinions. they are a quick and understandable way in which people can communicate how good of an experience they had, usually with five stars being the highest score and 0 being the worst. For this reason, a good star rating can be pivotal for businesses that aim to attract customers, which makes Yelp - an online review database and website - important. In this analysis, we aim to understand what motivates a user to choose one specific star rating over others, as understanding this might help business owners when trying to elevate their establishment reviews. The CRISP-DM methodology and three different methods were used: linear regression, lasso, and random forest with data provided by Yelp in various data sets, which detail information on businesses, user reviews, user profiles, check-ins, and user tips.


**Analytical Approach**

As stated above, what this analysis aimed to extrapolate from the data was an understanding of what factors influence users to choose any given score when reviewing, as that would help in knowing what leads to high-scoring reviews. The data gives us insight into many of the specifics of each business - from the type of establishment to their operational schedule - as well as the specifics of each review and user. Examining the details of each business as well as the specific words of the reviews seems to be the best way this data can give relevant information. This project aims to extract any variables from the data that may influence star ratings.


**Data Collection, Distribution, Understanding, and Preparation**

This analysis used Yelp-provided data. Smaller versions of the sets detailing user-profiles and reviews were used for this reason. The review data set had 1398056 observations, which presented many processing complications later on when performing sentiment analysis. Using the "categories" section in the data set, the top categories are identified (shown in the figure below), for the top category (Restaurants) to be used in the experiment. Using different categories might influence the model negatively - as the variables that might influence a review for a restaurant are different than the variables of a doctor's office. This accomplished the secondary objective of reducing the pool of reviews to analyze, and while it may influence the outcome of the model it is a necessary processing step. It is noticeable that there is a skewed distribution towards either 1 star or five stars. This distribution is very similar to the distribution of star reviews in the larger data set, signifying that it is not a specific attribute of restaurants and that the simplified data was accurate.

The business information data set included details of each establishment: name, address, location, what category of establishment it fits, specific attributes, average star rating, and operational schedule. Most of these were cleaned by turning their data into numerical variables easier to work with. Additionally, some data was simplified (operational schedule into “Is the establishment closed on weekends?”). Finally, some unnecessary or repeated variables were eliminated, like the different location variables or mostly empty attribute columns. 

Many columns in User data were eliminated, as they were deemed not necessary for the analysis. Finally, data in "tip_data" and "checkin_data" was largely discarded. Information about how often a user uses the app or special tips about establishments is not important for scoring a review. After this was done, the remaining data was compiled in a merged data set that included all relevant variables.


**Experiment and Modeling**

After unsuccessful first attempts at creating a linear model, user sentiment scores are extracted by performing sentiment analysis on the text of each review. This is a process to extract the writer's feelings when writing a text by comparing the vocabulary used with lexicons. By assigning a sentiment score to each review, it may prove that the mood and/or specifics of the review experience are influential. 
The analysis worked through three versions of models. Version One utilized only sentiment analysis scores to predict the star ratings, which showed promising results. The second version included all available variables, which caused concerns for over-fitting and the viability of the data - as the aim of this analysis is to find what influences reviews, narrowing this down to a few variables would be more beneficial for the establishments to manage and control. Finally, this led to a third iteration of the model using the five variables with the most importance in determining star ratings. This was found through a Random Forest model, and later these variables were fitted in linear and lasso models.


**Experiment Results**

The analysis led to the following results, expressed in the tables below. 

![Results Summary](https://github.com/bernie-being/myrepo/blob/e6ef5c7c319342ee9eb23d4d775eb17fa1e3f222/SummaryTableR.png)

Version 1 refers to a linear model using only the sentiment scores performed, yielding a small p-value and a Multiple R-squared of 0.1988, which is not a satisfactory result as the model can only explain 19% of the data. A lasso model was used for comparison, which confirmed a 1.114354 mean average error. However, Version Two managed to explain 46% of the variation in the star values. Further research on the subject is needed to expand this model, but this analysis narrowed down five variables that explain up to 43% of star scores in reviews through the third version of this model. This third model offered a Mean Average Error of 0.8269731, which while not perfect, is still within one star value of error and could be utilized effectively to predict star scoring. 

A random forest model was utilized to compare the importance of these variables and at the same time test the accuracy of a model including all variables. 

This testing revealed that the top five more important variables in the model are, in order: 

1. "user_average_stars_given"
2. "bing_score"
3. "afinn_score"
4. "user_review_count"
5. "Business_Average_Stars".

This gives us insight into the more important factors that establishments should take into account if they decide to look at/change their reviews.


**Conclusions**

In this analysis, we predicted the star scores of restaurants using Yelp-provided data and developed a model that managed to predict accurately within one star of error. From this model, some conclusions can be drawn. Establishments, specifically restaurants, that aim to understand what affects their star ratings on Yelp should look at the main 5 variables found in this analysis.

While establishments cannot specifically choose frequent reviewers that give higher stars on average, they can advertise directly to them through the Yelp Ads service to increase their chances. Additionally, it was shown that sentiment/emotions in a reviewer are important when deciding a score, so influencing it through attitude, service, ambiance, and decorations will prove useful. Finally, previous establishment performance and herd mentality also influence star scores - people are more likely to review an establishment lower if that establishment's reviews are already down, which means that for businesses a good start and maintaining that good star rating momentum is imperative. 

While the modeling was successful, it was by no means perfect or exhaustive. Future works in this area may expand on many areas, for example, working with a larger data set with a more balanced star distribution - two things that might have influenced the model here. Future work should strive to expand on the variables found today and add to them with more data, in other business categories and types to not only relegate this analysis on restaurants. 

### Code / R Script Used

